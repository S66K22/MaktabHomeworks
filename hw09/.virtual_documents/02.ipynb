


import pandas as pd
import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from collections import defaultdict
import time





df = pd.read_csv('diabetic_data.csv')
df.shape


df.isna().sum()


(df.shape[0] - df[['max_glu_serum']].isna().sum()) / df.shape[0]


(df.shape[0] - df[['A1Cresult']].isna().sum()) / df.shape[0]


# remove max_glu_serum and A1Cresult
df.dropna(axis=1, inplace=True)
df.shape


df.isna().sum()


# check columns got "?" values
df.astype(str).eq("?").sum()


# remove weight. it has big number of "?" values
df.drop(columns=['weight'], inplace=True)


df.shape


df.info()


for column in ['race', 'payer_code', 'medical_specialty', 'diag_1', 'diag_2', 'diag_3', 'age']:
    print("*" * 10)
    print(f"column {column} unique values: {df[column].unique()}")


df.readmitted.unique()


# remove rows containing values starting with letters
for column in ['diag_1', 'diag_2', 'diag_3']:
    df = df[~df[column].str.startswith(('E', 'V'))]
df.shape


bin_midpoints = {
    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,
    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,
    '[80-90)': 85, '[90-100)': 95
}

df['age_numeric'] = df['age'].map(bin_midpoints)
df.drop(columns=['age'], inplace=True)


df['readmitted_binary'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)

y = df['readmitted_binary']
print(df['readmitted_binary'].value_counts())
X = df.drop(columns=['readmitted', 'readmitted_binary'])
X.shape, y.shape


categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_features


numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.to_list()
numeric_features


# the pipline uses imputer to replace ? with mose frequent values
preprocessor = ColumnTransformer([
        ("num", SimpleImputer(strategy="median"), numeric_features), 
        ("cat", Pipeline([
            ("imputer", SimpleImputer(missing_values="?", strategy="most_frequent")),
            ("encoder", OneHotEncoder(handle_unknown="ignore"))
        ]), categorical_features),
    ]
)

# Fit and transform the DataFrame
X_processed = preprocessor.fit_transform(X)
X_processed.shape


# For numeric features, their names remain the same
num_features_names = numeric_features

# For categorical features, get the names from the OneHotEncoder
cat_transformer = preprocessor.named_transformers_['cat']
ohe = cat_transformer.named_steps['encoder']

# This handles underscores in original feature names correctly
cat_features_names = ohe.get_feature_names_out(categorical_features)

# Combine numeric + one-hot encoded categorical features
all_feature_names = np.concatenate([num_features_names, cat_features_names])
all_feature_names.shape





X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)


models = {}

for leaves in [100, 300]:
    models[f"rf_{leaves}"] = RandomForestClassifier(n_estimators=100, max_leaf_nodes=leaves, random_state=42)
    models[f"xgb_{leaves}"] = XGBClassifier(
                                    n_estimators=100,
                                    max_leaves=leaves,
                                    learning_rate=0.1,
                                    random_state=42
                                )


print("training...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"training {name}\n")
    model.fit(X_train, y_train)
    print(f"it took {time.time() - start_time} seconds to train model {name}")
    print("*" * 10)
    print()



print("predicting...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_pred = model.predict(X_test)
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()



values, count = np.unique(y_test, return_counts=True)
print("y_test: ", values, count)
values, count = np.unique(y_pred, return_counts=True)
print("y_pred: ",values, count)


print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.2
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()






models = {}

# scale_pos_weight = (#negative samples) / (#positive samples)
scale = (y_train == 0).sum() / (y_train == 1).sum()

for leaves in [100, 300]:
    models[f"rf_{leaves}"] = RandomForestClassifier(n_estimators=100, max_leaf_nodes=leaves, class_weight='balanced', random_state=42)
    models[f"xgb_{leaves}"] = XGBClassifier(
                                    n_estimators=100,
                                    max_leaves=leaves,
                                    learning_rate=0.1,
                                    scale_pos_weight=scale,
                                    random_state=42
                                )


print("training...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"training {name}\n")
    model.fit(X_train, y_train)
    print(f"it took {time.time() - start_time} seconds to train model {name}")
    print("*" * 10)
    print()



print("predicting...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_pred = model.predict(X_test)
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()



print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.3
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()



import lime
from lime.lime_tabular import LimeTabularExplainer

explainer = LimeTabularExplainer(
    training_data=X_train,
    feature_names=all_feature_names,
    class_names=['No disease','Disease'],
    mode='classification'
)

exp = explainer.explain_instance(X_test[0], models['rf_300'].predict_proba)
exp.show_in_notebook(show_table=True)



from sklearn.inspection import PartialDependenceDisplay

pipeline = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("classifier", models['rf_300'])
])

PartialDependenceDisplay.from_estimator(
    pipeline,          # preprocessor + model
    X_test_raw,         # original DataFrame
    features=[
        "time_in_hospital",
        "num_medications",
        "num_lab_procedures"
    ]
)








smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)


smote_models = {}

for leaves in [100, 300]:
    smote_models[f"rf_{leaves}"] = RandomForestClassifier(n_estimators=100, max_leaf_nodes=leaves, random_state=42)
    smote_models[f"xgb_{leaves}"] = XGBClassifier(
                                    n_estimators=100,
                                    max_leaves=leaves,
                                    learning_rate=0.1,
                                    random_state=42
                                )


print("training...\n")
for name, model in smote_models.items():
    start_time = time.time()
    print(f"training {name}\n")
    model.fit(X_train, y_train)
    print(f"it took {time.time() - start_time} seconds to train model {name}")
    print("*" * 10)
    print()



print("predicting...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_pred = model.predict(X_test)
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()



print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.1
for name, model in smote_models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()






importances = models['rf_100'].feature_importances_



feature_importances = pd.DataFrame({
    'feature': all_feature_names,
    'importance': importances
}).sort_values(by='importance', ascending=False)

feature_importances.head(20)


important_features = feature_importances.feature[:20].tolist()
important_features_copy = []
for name in important_features:
    if name not in X.columns.tolist():
        print(name)
    else:
        important_features_copy.append(name)

for name in ['diag_3', 'diag_2', 'insulin', 'diabetesMed']:
    important_features_copy.append(name)

len(important_features_copy)


numeric_features_copy = []
categorical_features_copy = []

for name in important_features_copy:
    if name in num_features_names:
        numeric_features_copy.append(name)
    elif name in categorical_features:
        categorical_features_copy.append(name)


len(numeric_features_copy), len(categorical_features_copy)


numeric_features_copy


categorical_features_copy



# the pipline uses imputer to replace ? with mose frequent values
preprocessor = ColumnTransformer([
        ("num", SimpleImputer(strategy="median"), numeric_features_copy), 
        ("cat", Pipeline([
            ("imputer", SimpleImputer(missing_values="?", strategy="most_frequent")),
            ("encoder", OneHotEncoder(handle_unknown="ignore"))
        ]), categorical_features_copy),
    ]
)

# Fit and transform the DataFrame
X_processed = preprocessor.fit_transform(X)
X_processed.shape





X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)


models = {}

# scale_pos_weight = (#negative samples) / (#positive samples)
scale = (y_train == 0).sum() / (y_train == 1).sum()

for leaves in [100, 300]:
    models[f"rf_{leaves}"] = RandomForestClassifier(n_estimators=100, max_leaf_nodes=leaves, class_weight='balanced', random_state=42)
    models[f"xgb_{leaves}"] = XGBClassifier(
                                    n_estimators=100,
                                    max_leaves=leaves,
                                    learning_rate=0.1,
                                    scale_pos_weight=scale,
                                    random_state=42
                                )


print("training...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"training {name}\n")
    model.fit(X_train, y_train)
    print(f"it took {time.time() - start_time} seconds to train model {name}")
    print("*" * 10)
    print()




print("predicting...\n")
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_pred = model.predict(X_test)
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()



print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.3
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    sensitivity = recall_score(y_test, y_pred, pos_label=1)  # 1 = positive class
    print(f"Sensitivity of {name}: {sensitivity:.20f}")
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()





from joblib import dump, load

# Save the trained model (and optionally preprocessor)
for name, model in models.items():
    saved_model_name = f"{name}.joblib"
    dump(model, saved_model_name)

dump(preprocessor, 'preprocessor.joblib')
dump(num_features_names, "num_features_names.joblib")
dump(cat_features_names, "cat_features_names.joblib")


# Later, load it back
# clf_loaded = load('random_forest_model.joblib')
# preprocessor_loaded = load('preprocessor.joblib')


print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.3
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_train)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    print(f'classification_report: {classification_report(y_train, y_pred)}')
    auc = roc_auc_score(y_train, y_proba)
    print("ROC-AUC:", auc)
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()


print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.3
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    print(f'classification_report: {classification_report(y_test, y_pred)}')
    auc = roc_auc_score(y_test, y_proba)
    print("ROC-AUC:", auc)
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()


print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.5
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_train)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    print(f'classification_report: {classification_report(y_train, y_pred)}')
    auc = roc_auc_score(y_train, y_proba)
    print("ROC-AUC:", auc)
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()


print("use lover threshold instead of default 0.5 threshold.")
print("predicting...\n")
threshold = 0.5
for name, model in models.items():
    start_time = time.time()
    print(f"predicting {name}\n")
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)  # lower threshold
    print(f'classification_report: {classification_report(y_test, y_pred)}')
    auc = roc_auc_score(y_test, y_proba)
    print("ROC-AUC:", auc)
    print(f"it took {time.time() - start_time} seconds to predicting model {name}")
    print("*" * 10)
    print()






